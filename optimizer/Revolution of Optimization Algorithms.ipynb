{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Optimizaition Algorithms Notebook\n",
    "\n",
    "@Author - Hoang Vu Trong Thuy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Standard Gradient Descent (GD)\n",
    "2. Stochastic Gradient Descent (SGD)\n",
    "3. Mini-batch Gradient Descent (MGD)\n",
    "4. GD with Momentum\n",
    "5. Nesterov accelerated gradient (NAG)\n",
    "\n",
    "----- \n",
    "Learning Rate Adapting\n",
    "6. Adagrad\n",
    "7. Adadelta\n",
    "8. Adam\n",
    "9. RMProps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*x + 10*np.cos(x)\n",
    "\n",
    "def has_converged(theta, grad_f):\n",
    "    return abs(grad_f(theta)) < 1e-3\n",
    "\n",
    "def get_default_function():\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "    ax = fig.add_subplot(111, frameon=False)\n",
    "\n",
    "    # Create the mesh in polar coordinates and compute corresponding Z.\n",
    "    X = np.linspace(-10, 10, 50)\n",
    "    Y = f(X)\n",
    "\n",
    "    # Plot the function.\n",
    "    ax.plot(X, Y, lw=5)\n",
    "\n",
    "    # Tweak the limits and add latex math labels.\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$y = x^2 + 10sin(x)$')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "def generate_gif(fname, fig, animate, frames, interval):\n",
    "    animate_gif = FuncAnimation(fig,animate,frames=frames,interval=interval)\n",
    "    animate_gif.save(fname, writer='imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standard Gradient Descent\n",
    "\n",
    "$$\\theta_{n+1} = \\theta_n - \\eta \\nabla_{\\theta}J(\\theta_n)$$\n",
    "\n",
    "- Advantages:\n",
    "    - ...\n",
    "- Disvantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_gradient_descent(grad_f, learning_rate=0.01, init_theta=10.0):\n",
    "    cache = [init_theta]\n",
    "    for it in range(100):\n",
    "        new_theta = cache[-1] - learning_rate * grad_f(cache[-1])\n",
    "        if has_converged(new_theta, grad_f):\n",
    "            break\n",
    "        \n",
    "        cache.append(new_theta)\n",
    "    \n",
    "    return cache, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, it = std_gradient_descent(grad_f, learning_rate=0.01)\n",
    "f_cache = f(np.array(cache))\n",
    "\n",
    "fig, axis = get_default_function()\n",
    "\n",
    "points = axis.scatter(cache[0], f_cache[0], linewidths=10, c='r')\n",
    "\n",
    "def std_animate(i):\n",
    "    points.set_offsets([cache[i], f_cache[i]])\n",
    "    axis.set_title('Iterate {}'.format(i))\n",
    "    return points, axis\n",
    "\n",
    "generate_gif('std_GD.gif', fig, std_animate, frames=len(cache), interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stochastic Gradient Descent\n",
    "\n",
    "Instead of updating all params, SGD only use 1 sample $\\theta$ to compute gradient $\\nabla_{\\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent \\w Momentum \n",
    "\n",
    "A big problem of Standard Gradient Descent (StdGD) is its optimal point not always globally. A step gets over this gap was attach our point a velocity, similar to a ball rolling down the hill. This basic idea was core concept of below algorithms.\n",
    "\n",
    "$$v_{t}= \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta)$$\n",
    "$$\\theta = \\theta - v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gradient_descent(grad_f, gamma=0.9, learning_rate=0.01 , init_theta = 10.0):\n",
    "    caches = [init_theta]\n",
    "    velocities = [0]\n",
    "    \n",
    "    for it in range(200):\n",
    "        velocity = gamma * velocities[-1] + learning_rate * grad_f(caches[-1])\n",
    "        new_theta = caches[-1] - velocity\n",
    "        \n",
    "        if has_converged(new_theta, grad_f) and abs(velocity) < 1e-1:\n",
    "            break\n",
    "        \n",
    "        caches.append(new_theta)\n",
    "        velocities.append(velocity)\n",
    "    \n",
    "    return caches, velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgd_cache, mgd_veloc = momentum_gradient_descent(grad_f)\n",
    "mgd_fcache = f(np.array(mgd_cache))\n",
    "\n",
    "fig, axis = get_default_function()\n",
    "points = axis.scatter(cache[0], f_cache[0], linewidths=10, c='r')\n",
    "\n",
    "def momentum_animate(i):\n",
    "    points.set_offsets([cache[i], f_cache[i]])\n",
    "    axis.set_title('Iterate {}'.format(i))\n",
    "    return points, axis\n",
    "\n",
    "\n",
    "generate_gif('momentum_GD.gif', fig, momentum_animate, frames=len(mgd_cache), interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
